---
title: "Assignment 7"
author: "Charles Lang"
date: "12/20/2017"
output: html_document
---

In the following assignment you will be looking at data from an one level of an online geography tutoring system used by 5th grade students. The game involves a pre-test of geography knowledge (pre.test), a series of assignments for which you have the average score (av.assignment.score),  the number of messages sent by each student to other students about the assignments (messages), the number of forum posts students posted asking questions about the assignment (forum.posts), a post test at the end of the level (post.test) and whether or not the system allowed the students to go on to the next level (level.up).  

```{r}
library(rpart)
```

```{r}
DF.ORIG <- read.csv("online.data.csv")

DF2 <- DF.ORIG[ ,-1]
summary(DF2) # no NA in the level.up column 
DF2$level.up <- as.character(DF2$level.up) 
DF2$level.up <- ifelse(DF2$level.up == "no", 0, 1) # recoded as 1, 0
```

```{r}
library(ggplot2)
library(reshape2)
g <- ggplot(data = melt(DF2), mapping = aes(x = value)) + 
    geom_histogram(bins = 20) + facet_wrap(~variable, scales = 'free_x')
g
pairs(DF2) #there could be some positive relations
```

```{r}
#Create a classification tree
library(rpart)
tree1 <- rpart(level.up ~ post.test.score + messages + av.assignment.score, method="class", control=rpart.control(minsplit=5, cp=0.00001), data = DF.ORIG[ ,-1])
#Plot and generate a CP table for your tree 
printcp(tree1)
post(tree1, file = "tree1.ps", title = "tree1: predict if student level up")
bestcp <- tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"]
```

```{r}
tree1.pruned <- prune(tree1, cp = bestcp)

DF3 <- DF.ORIG[,-1] 
DF3$pred <- predict(tree1.pruned, type = "prob")[,2]
```

```{r}
#Now you can generate the ROC curve for your model. You will need to install the package ROCR to do this.
#install.packages("ROCR")
library(ROCR)

#Plot the curve
pred.detail <- prediction(DF3$pred, DF3$level.up) 

plot(performance(pred.detail, "tpr", "fpr"))
abline(0, 1, lty = 2)
```

```{r}
#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail,"auc"), "y.values"))#Unlist liberates the AUC value from the "performance" object created by ROCR

```

```{r}
DF4 <- DF.ORIG[, -1]
tree2 <- rpart(level.up ~ pre.test.score + forum.posts, method="class", control=rpart.control(minsplit=5, cp=0.00001), data = DF4)

printcp(tree2)
post(tree2, file = "tree2.ps", title = "tree1: predict if student level up")
bestcp2 <- tree2$cptable[which.min(tree2$cptable[,"xerror"]),"CP"]
```

```{r}
tree2.pruned <- prune(tree2, cp = bestcp2)

DF5 <- DF.ORIG[,-1] # copy from DF.ORIGIN 
DF5$pred <- predict(tree2.pruned, type = "prob")[,2]
```

```{r}
# ROC & AUC for `tree2.pruned`
pred.detail2 <- prediction(DF5$pred, DF5$level.up) 

plot(performance(pred.detail2, "tpr", "fpr"))
abline(0, 1, lty = 2)

# AUC
unlist(slot(performance(pred.detail2,"auc"), "y.values"))  # AUC of tree2 is: 0.8180375
```
`tree1` outperforms `tree2` regarding their AUC values(1 > 0.82) so according to the definition of AUC, `tree1` is a better model.

```{r}
## estimate accuracy
acc.perf = performance(pred.detail2, measure = "acc")
plot(acc.perf) ## optimal threshold ranges from 0.4-0.65 

## Another way to find the cutting point for the optimal probability threshold (supposed the cost for false positive is the same as false negative)
cost.perf = performance(pred.detail2, "cost")
pred.detail2@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
## probability threshold: 0.64

# set cut.off1 = 0.3
threshold.pred1 <- predict(tree2.pruned, type = "prob")[,2]
threshold.pred1 <- ifelse(threshold.pred1 < 0.3, "no", "yes")
threshold.pred1 <- as.factor(threshold.pred1)
#Now generate three diagnostics:

D1$accuracy.model1 <-

D1$precision.model1 <- 

D1$recall.model1 <- 

#Finally, calculate Kappa for your model according to:

#First generate the table of comparisons
DF6 <- DF.ORIG        
DF6$threshold.pred1 <- threshold.pred1
table1 <- table(DF6$level.up, DF6$threshold.pred1)

#Convert to matrix
matrix1 <- as.matrix(table1)

#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
# kappa (threshold: 0.3) is : 0.86

#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?

threshold.pred2 <- predict(tree2.pruned, type = "prob")[,2]
threshold.pred2 <- ifelse(threshold.pred2 < 0.6, "no", "yes")
threshold.pred2 <- as.factor(threshold.pred2)
DF7 <- DF.ORIG        
DF7$threshold.pred2 <- threshold.pred2
table2 <- table(DF7$level.up, DF7$threshold.pred2)
matrix2 <- as.matrix(table2)
kappa(matrix2, exact = TRUE)/kappa(matrix2)
# kappa (threshold: 0.6) is : 1.05 (this is obviously wrong)


```

```{r}
library(dplyr)
test <- DF4[c(1:5), c(1:3)]
n <- c(rep(NA, 3))
test <- rbind(test, n)

```

